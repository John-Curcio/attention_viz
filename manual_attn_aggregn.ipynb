{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe4e14a-d4ff-44b4-99f7-888bbd3529ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.24.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (4.25.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.24.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q sentencepiece\n",
    "\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "\n",
    "!pip install scipy\n",
    "!pip install protobuf\n",
    "!pip install scipy\n",
    "!pip install tqdm\n",
    "# !python -m pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70b9b23-bc0e-459f-8ab4-a145cf7b734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6011ea2c7e42a394c0a71f7423f6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:233: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, \\\n",
    "    BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "adapters_name = \"model_outputs/dna_computing_model/\"\n",
    "model_name = \"openlm-research/open_llama_7b\"\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=nf4_config,\n",
    "    output_attentions=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapters_name,\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0df48db-8c73-4ee7-b34f-0870481900fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3748 MiB |   3942 MiB |  42133 MiB |  38384 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3748 MiB |   3942 MiB |  42133 MiB |  38384 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3726 MiB |   3919 MiB |  42074 MiB |  38347 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   4118 MiB |   4118 MiB |   4204 MiB |  88064 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 144615 KiB | 204109 KiB |  14751 MiB |  14610 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1285    |    1668    |    5253    |    3968    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1285    |    1668    |    5253    |    3968    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     153    |     153    |     154    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      93    |      98    |    1215    |    1122    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mem_check():\n",
    "    print(torch.cuda.memory_summary(abbreviated=True))\n",
    "\n",
    "mem_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b5e4c9-c1b5-4b6b-968b-0da1d67d9e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16608\n"
     ]
    }
   ],
   "source": [
    "with open(\"DNA_computing.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88d6bfc-a366-4328-89e7-a3b8be40fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19488/90304542.py:3: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def aggregate_attention(attn):\n",
    "    '''Extract average attention vector'''\n",
    "    avged = []\n",
    "    for layer in attn:\n",
    "        layer_attns = layer.squeeze(0)\n",
    "        attns_per_head = layer_attns.mean(dim=0)\n",
    "        vec = torch.concat((\n",
    "            # We zero the first entry because it's what's called\n",
    "            # null attention (https://aclanthology.org/W19-4808.pdf)\n",
    "            torch.tensor([0.]),\n",
    "            # usually there's only one item in attns_per_head but\n",
    "            # on the first generation, there's a row for each token\n",
    "            # in the prompt as well, so take [-1]\n",
    "            attns_per_head[-1][1:].to(\"cpu\"),\n",
    "            # add zero for the final generated token, which never\n",
    "            # gets any attention\n",
    "            torch.tensor([0.]),\n",
    "        ))\n",
    "        avged.append(vec / vec.sum())\n",
    "    return torch.stack(avged).mean(dim=0)\n",
    "    \n",
    "def precompute_attns(text):\n",
    "    # right now I'm not worried about batch inference\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    result = []\n",
    "    for n in tqdm(range(2, len(inputs[0]))):\n",
    "        outputs = model(\n",
    "            inputs[:,:n],\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        result.append(aggregate_attention(outputs.attentions))\n",
    "    return result\n",
    "\n",
    "# attns = precompute_attns(\"DNA computing is an emerging branch of unconventional computing which uses DNA, biochemistry, and molecular\")\n",
    "# attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78f7461-5891-4608-a6cc-c0537df81a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8037b3b6-61db-4eba-8782-d28b9c6e3b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3606 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a696244-16cf-4baf-9a4c-be9360756f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# temp_outputs = model(\n",
    "#     inputs[:,:2048],\n",
    "#     # output_attentions=True,\n",
    "#     output_hidden_states=False,\n",
    "#     past_key_values=None,\n",
    "#     # use_cache=False,\n",
    "# )\n",
    "# end = time.time()\n",
    "# print(end - start)\n",
    "# temp_outputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b894c5e7-8645-44f4-bb79-47952c885673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0fac90f-ad1c-46ba-8528-696d68c6eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_inputs = inputs[:,:2048]\n",
    "token_embeds = model.model.embed_tokens(temp_inputs)\n",
    "fee = model.model.layers[0](token_embeds, output_attentions=True)\n",
    "\n",
    "# TODO: aggregate attention while going through the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d52798-2ee2-40a2-aea2-8cb415a18770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2048, 4096]), torch.Size([1, 32, 2048, 2048]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fee[0].shape, fee[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c03ae9-1339-40ec-9557-f878cd532b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True,  ...,  True,  True,  True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_info_tokens = [tokenizer.encode(token)[1] for token in [\",\", \"the\", \"of\"]]\n",
    "low_info_tokens.append(31876)\n",
    "\n",
    "has_info = inputs[0] != torch.tensor(low_info_tokens).reshape(-1,1).all(0)\n",
    "has_info[0] = 0 # null attention\n",
    "has_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17538b8b-020e-4320-9162-b66bbf4db595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24239850044250488"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def agg_attn(inputs):\n",
    "    n = inputs.shape[1]\n",
    "\n",
    "    has_info = inputs[0] != torch.tensor(low_info_tokens).reshape(-1,1).all(0)\n",
    "    has_info[0] = 0 # null attention\n",
    "    has_info = has_info.to(\"cuda\")\n",
    "    \n",
    "    prev_outputs = model.model.embed_tokens(inputs)\n",
    "    avged = torch.zeros((n, n)).to(\"cuda\")\n",
    "    print(inputs.shape)\n",
    "    for layer in model.model.layers:\n",
    "        curr_outputs, curr_attns = layer(prev_outputs, output_attentions=True)\n",
    "        attns_per_head = curr_attns[-1].mean(dim=0)\n",
    "        # print(attns_per_head.shape, curr_attns[-1].shape)\n",
    "        # assert False\n",
    "        vec = attns_per_head * has_info\n",
    "        # assert curr_attns.sum() != 0\n",
    "        # assert (curr_attns != 0).any()\n",
    "        # assert (vec != 0).any()\n",
    "        avged += vec #/ vec.sum()\n",
    "        prev_outputs = curr_outputs\n",
    "    return avged\n",
    "        \n",
    "start = time.time()\n",
    "avged = agg_attn(inputs[:,:2048])\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657eacf6-22b1-400c-9c52-ad434ba6e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2166, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (avged != 0).any()\n",
    "max(avged.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307e5ace-3f83-42e0-b612-8af83e067b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2166, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avged.mean(0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f17b83cd-ff15-45a0-8b32-4e28523b3a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3606])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b53cecb-bd2a-4e7e-a2bf-b829331636bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 10, 20, 20)).squeeze(0).mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16e381c0-2ca9-4fb2-8b87-675aee9b18e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6005, device='cuda:0'), tensor(0.2166, device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avged.max(), avged.mean(0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55f89e58-4148-4a0f-8047-97a8c9dacf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 2048])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958241c-7181-4ebc-837d-b8eeff09d85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3476b71-7863-4f71-90c8-5598a1b9e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4052 MiB |   5428 MiB | 144629 MiB | 140576 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4052 MiB |   5428 MiB | 144629 MiB | 140576 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4030 MiB |   5406 MiB | 144539 MiB | 140508 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   6114 MiB |   6114 MiB |   6428 MiB | 321536 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 177373 KiB |    796 MiB |  83166 MiB |  82993 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1293    |    1668    |   10042    |    8749    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1293    |    1668    |   10042    |    8749    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     143    |     153    |     165    |      22    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      95    |      99    |    2632    |    2537    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4052 MiB |   5428 MiB | 144629 MiB | 140576 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4052 MiB |   5428 MiB | 144629 MiB | 140576 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4030 MiB |   5406 MiB | 144539 MiB | 140508 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   4226 MiB |   6114 MiB |   6428 MiB |   2202 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 177373 KiB |    796 MiB |  83166 MiB |  82993 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1293    |    1668    |   10042    |    8749    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1293    |    1668    |   10042    |    8749    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     136    |     153    |     165    |      29    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      95    |      99    |    2632    |    2537    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mem_check()\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa55cf03-acfd-4b9b-9926-ca733b92c9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([torch.zeros(10).reshape(1, -1).T, torch.ones(5).reshape(1, -1).T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d16b7cf7-fb4f-4bf6-9ec7-8b51c17365c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9ab50e3a-4f12-4d80-8277-710e6470b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 2/20 [00:00<00:01, 14.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 4/20 [00:00<00:01, 13.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 6/20 [00:00<00:01, 12.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 8/20 [00:00<00:00, 12.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 10/20 [00:00<00:00, 12.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 12/20 [00:00<00:00, 12.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 14/20 [00:01<00:00, 12.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 16/20 [00:01<00:00, 12.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 12.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 20/20 [00:01<00:00, 12.87it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_attns inputs.shape: 13\n",
      "torch.Size([14, 13])\n",
      "(14,) torch.Size([14, 13]) torch.Size([13]) 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: rgba(255, 0, 0, 0.14); \">DNA</span> <span style=\"background-color: rgba(255, 0, 0, 0.13); \">computing</span> <span style=\"background-color: rgba(255, 0, 0, 0.13); \">is</span> <span style=\"background-color: rgba(255, 0, 0, 1.00); \">an</span> <span style=\"background-color: rgba(255, 0, 0, 0.07); \">emerging</span> <span style=\"background-color: rgba(255, 0, 0, 0.06); \">branch</span> <span style=\"background-color: rgba(255, 0, 0, 0.12); \">of</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">uncon</span> <span style=\"background-color: rgba(255, 0, 0, 0.13); \">ventional</span> <span style=\"background-color: rgba(255, 0, 0, 0.39); \">computing</span> <span style=\"background-color: rgba(255, 0, 0, 0.64); \">which</span> <span style=\"background-color: rgba(255, 0, 0, 0.97); \">uses</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); text-decoration: underline;\">DNA</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">,</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">bi</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">ochemistry</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">,</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">and</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">molecular</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1362,  0.1380,  0.1292,  0.1275,  1.0000,  0.0733,  0.0595,  0.1178,\n",
      "         0.0000,  0.1285,  0.3858,  0.6426,  0.9728], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def heterogenous_stack(vecs):\n",
    "    '''Pad vectors with zeros then stack'''\n",
    "    max_length = max(v.shape[0] for v in vecs)\n",
    "    return torch.stack([\n",
    "        torch.concat((v, torch.zeros(max_length - v.shape[0])))\n",
    "        for v in vecs\n",
    "    ])\n",
    "\n",
    "class AttentionVisualizer:\n",
    "\n",
    "    def __init__(self, model, tokenizer, drop_low_info_tokens=True, copy_matt=False):\n",
    "        self.low_info_tokens = [tokenizer.encode(token)[1]\n",
    "            for token in [\n",
    "                \",\",\n",
    "                \"the\",\n",
    "                \"of\",\n",
    "                \"an\",\n",
    "                # \"a\",\n",
    "                # \"'\",\n",
    "                # \"DNA\", # test\n",
    "            ]\n",
    "        ]\n",
    "        # 31876 is the token for an apostrophe '\n",
    "        # If I pass in \"'\" to the encode function, it will be encoded as\n",
    "        # 910, which is different from what I get if there are characters preceding\n",
    "        # it\n",
    "        self.low_info_tokens += [31876]\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.drop_low_info_tokens = drop_low_info_tokens\n",
    "        self.copy_matt = copy_matt\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Don't have to worry about fine-tuning the model on text yet\n",
    "        \"\"\"\n",
    "        self.raw_text = text\n",
    "        self.tokens_encoded = self.tokenizer.encode(text, return_tensors=\"pt\")#.to(\"mps\")\n",
    "        self.tokens_decoded = [self.tokenizer.decode(token) for token in self.tokens_encoded[0]]\n",
    "        self._precompute_attns()\n",
    "\n",
    "    def filtered_aggregate_attention(self, inputs):\n",
    "        n = inputs.shape[1]\n",
    "        # padding doesn't seem to be the culprit\n",
    "        # inputs = torch.concat([inputs[0], \n",
    "        #                        torch.tensor([tokenizer.pad_token_id] * (2048 - n))])\\\n",
    "        #     .reshape(1,-1)\n",
    "        has_info = torch.ones(n)\n",
    "        if self.drop_low_info_tokens:\n",
    "            has_info = (inputs[0] != torch.tensor(self.low_info_tokens).reshape(-1,1)).all(0)\n",
    "        #print(has_info)\n",
    "        #print(\"low info tokens excluded:\", (~has_info).sum())\n",
    "        has_info[0] = 0 # null attention\n",
    "        # has_info[-1] = 0 # ignore the generated token\n",
    "        # if has_info.shape[0] > 6:\n",
    "        #     has_info[6] = 0 # test\n",
    "        has_info = has_info.to(\"cuda\")\n",
    "        \n",
    "        prev_outputs = model.model.embed_tokens(inputs)\n",
    "        if self.copy_matt:\n",
    "            avged = torch.zeros(n).to(\"cuda\")\n",
    "            for layer in model.model.layers:\n",
    "                curr_outputs, curr_attns = layer(prev_outputs, output_attentions=True)\n",
    "                attns_per_head = curr_attns[-1].mean(dim=0)\n",
    "                vec = attns_per_head[-1] * has_info\n",
    "                avged += vec / vec.sum()\n",
    "                prev_outputs = curr_outputs\n",
    "            return avged\n",
    "        else:\n",
    "            avged = torch.zeros((32, n, n)).to(\"cuda\")\n",
    "            for layer in model.model.layers:\n",
    "                curr_outputs, curr_attns = layer(prev_outputs, output_attentions=True)\n",
    "                # curr_attns is (1, 32, n, n)\n",
    "                attns_per_head = curr_attns[-1].mean(dim=0)\n",
    "                vec = attns_per_head * has_info\n",
    "                avged += vec\n",
    "            return avged.mean(dim=0).mean(dim=0)\n",
    "\n",
    "    def _precompute_attns(self):\n",
    "        n = len(self.tokens_encoded[0])\n",
    "        self.precomputed_attentions = [\n",
    "            self.filtered_aggregate_attention(\n",
    "                self.tokens_encoded[:,:m]\n",
    "            )\n",
    "            for m in tqdm(range(1, n+1))\n",
    "        ]\n",
    "        return None\n",
    "\n",
    "    def generate_attentions_for_selection(self, selected_start, n_selected=1):\n",
    "        assert n_selected == 1, \"Only one selection supported for now! (TODO)\"\n",
    "        # inputs = self.tokens_encoded[:, 0:(selected_start)]\n",
    "        # print(self.tokens_decoded)\n",
    "        # inputs = self.tokens_decoded[:, 0:selected_start]\n",
    "        inputs = self.tokens_decoded[0:selected_start]\n",
    "        print(\"generate_attns inputs.shape:\", len(inputs))\n",
    "        attn_m = heterogenous_stack(\n",
    "            [\n",
    "                torch.tensor([\n",
    "                    1 if i == j else 0\n",
    "                    for j in range(selected_start)\n",
    "                ])\n",
    "                for i in range(selected_start)\n",
    "            ] +\n",
    "            # list(map(aggregate_attention, outputs.attentions))\n",
    "            # [self.filtered_aggregate_attention(inputs, outputs.attentions[0])]\n",
    "            # [self.precomputed_attentions[selected_start+1]]\n",
    "            [self.precomputed_attentions[selected_start-1].to(\"cpu\")]\n",
    "        )\n",
    "        return attn_m\n",
    "\n",
    "    def viz(self, selected_start, n_selected=1):\n",
    "        \"\"\"\n",
    "        Visualize attention for a given selection\n",
    "        \"\"\"\n",
    "        assert n_selected == 1, \"Only one selection supported for now! (TODO)\"\n",
    "        attn_m = self.generate_attentions_for_selection(selected_start, n_selected)\n",
    "\n",
    "        # Create a vector based on selected tokens\n",
    "        # selected_vec = np.zeros(len(self.tokens_encoded[0]))\n",
    "        print(attn_m.shape)\n",
    "        selected_vec = np.zeros(attn_m.shape[0])\n",
    "        selected_vec[selected_start:(selected_start + n_selected)] = 1\n",
    "\n",
    "        # Calculate attention vector\n",
    "        # attn_vec = np.dot(vec, attn_matrix.T)\n",
    "        attn_vec = np.matmul(selected_vec, attn_m)\n",
    "        min_val, max_val = min(attn_vec[1:]), max(attn_vec)\n",
    "        attn_vec = (attn_vec - min_val) / (max_val - min_val)\n",
    "        # Generate the HTML code for each token\n",
    "        spans = []\n",
    "        print(selected_vec.shape, attn_m.shape, attn_vec.shape, len(self.tokens_decoded))\n",
    "        for i in range(0, len(self.tokens_decoded)):\n",
    "            token = self.tokens_decoded[i]\n",
    "            if token == \"<s>\":\n",
    "                # think this is a special token. Possibly the start of the prompt?\n",
    "                continue\n",
    "            attn = 0\n",
    "            selected = False\n",
    "            if i < attn_vec.shape[0]:\n",
    "                attn = attn_vec[i]\n",
    "                #selected = False if i >= len(selected_vec) else selected_vec[i] == 1\n",
    "            underline_style = \"text-decoration: underline;\" if i == selected_start else \"\"\n",
    "            spans.append(f'<span style=\"background-color: rgba(255, 0, 0, {attn:.2f}); {underline_style}\">{token}</span>')\n",
    "\n",
    "        # Join the spans and display\n",
    "        display(HTML(' '.join(spans)))\n",
    "\n",
    "        # print(attn_matrix)\n",
    "        print(attn_vec)\n",
    "\n",
    "text_sub = \"DNA computing is an emerging branch of unconventional computing which uses DNA, biochemistry, and molecular\"\n",
    "\n",
    "viz = AttentionVisualizer(model, tokenizer, drop_low_info_tokens=False, copy_matt=True)\n",
    "viz.fit(text_sub)\n",
    "viz.viz(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed84a6-aed9-484e-a8df-736164106291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28f2d2e4-e9a7-4d33-a8f9-216c4c7c098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1] doesn't match the broadcast shape [2048]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m viz \u001b[38;5;241m=\u001b[39m AttentionVisualizer(model, tokenizer, drop_low_info_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, copy_matt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mviz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m viz\u001b[38;5;241m.\u001b[39mviz(\u001b[38;5;241m13\u001b[39m)\n",
      "Cell \u001b[0;32mIn[91], line 44\u001b[0m, in \u001b[0;36mAttentionVisualizer.fit\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#.to(\"mps\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_decoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_encoded[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_precompute_attns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 82\u001b[0m, in \u001b[0;36mAttentionVisualizer._precompute_attns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_precompute_attns\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     81\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_encoded[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecomputed_attentions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiltered_aggregate_attention(\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_encoded[:,:m]\n\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     87\u001b[0m     ]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[91], line 83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_precompute_attns\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     81\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_encoded[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecomputed_attentions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiltered_aggregate_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     87\u001b[0m     ]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[91], line 67\u001b[0m, in \u001b[0;36mAttentionVisualizer.filtered_aggregate_attention\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     attns_per_head \u001b[38;5;241m=\u001b[39m curr_attns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     66\u001b[0m     vec \u001b[38;5;241m=\u001b[39m attns_per_head[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m has_info\n\u001b[0;32m---> 67\u001b[0m     avged \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m vec \u001b[38;5;241m/\u001b[39m vec\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     68\u001b[0m     prev_outputs \u001b[38;5;241m=\u001b[39m curr_outputs\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avged\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1] doesn't match the broadcast shape [2048]"
     ]
    }
   ],
   "source": [
    "viz = AttentionVisualizer(model, tokenizer, drop_low_info_tokens=False, copy_matt=True)\n",
    "viz.fit(text_sub)\n",
    "viz.viz(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39fc10d5-f762-4d3a-9080-f15a31b552c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_attentions.shape 32 torch.Size([20, 32, 20, 20])\n",
      "generate_attns inputs.shape: 13\n",
      "torch.Size([14, 21])\n",
      "(14,) torch.Size([14, 21]) torch.Size([21]) 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: rgba(255, 0, 0, 0.96); \">DNA</span> <span style=\"background-color: rgba(255, 0, 0, 1.00); \">computing</span> <span style=\"background-color: rgba(255, 0, 0, 0.60); \">is</span> <span style=\"background-color: rgba(255, 0, 0, 0.49); \">an</span> <span style=\"background-color: rgba(255, 0, 0, 0.39); \">emerging</span> <span style=\"background-color: rgba(255, 0, 0, 0.37); \">branch</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">of</span> <span style=\"background-color: rgba(255, 0, 0, 0.34); \">uncon</span> <span style=\"background-color: rgba(255, 0, 0, 0.34); \">ventional</span> <span style=\"background-color: rgba(255, 0, 0, 0.49); \">computing</span> <span style=\"background-color: rgba(255, 0, 0, 0.49); \">which</span> <span style=\"background-color: rgba(255, 0, 0, 0.55); \">uses</span> <span style=\"background-color: rgba(255, 0, 0, 0.88); text-decoration: underline;\">DNA</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">,</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">bi</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">ochemistry</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">,</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">and</span> <span style=\"background-color: rgba(255, 0, 0, 0.00); \">molecular</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.9586, 1.0000, 0.5960, 0.4947, 0.3948, 0.3723, 0.0000, 0.3422,\n",
      "        0.3442, 0.4874, 0.4911, 0.5482, 0.8760, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def heterogenous_stack(vecs):\n",
    "    '''Pad vectors with zeros then stack'''\n",
    "    max_length = max(v.shape[0] for v in vecs)\n",
    "    return torch.stack([\n",
    "        torch.concat((v, torch.zeros(max_length - v.shape[0])))\n",
    "        for v in vecs\n",
    "    ])\n",
    "\n",
    "class AttentionVisualizer:\n",
    "\n",
    "    def __init__(self, model, tokenizer, drop_low_info_tokens=True):\n",
    "        self.low_info_tokens = [tokenizer.encode(token)[1]\n",
    "            for token in [\n",
    "                \",\",\n",
    "                \"the\",\n",
    "                \"of\",\n",
    "                # \"'\",\n",
    "            ]\n",
    "        ]\n",
    "        # 31876 is the token for an apostrophe '\n",
    "        # If I pass in \"'\" to the encode function, it will be encoded as\n",
    "        # 910, which is different from what I get if there are characters preceding\n",
    "        # it\n",
    "        self.low_info_tokens.append(31876)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.drop_low_info_tokens = drop_low_info_tokens\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Don't have to worry about fine-tuning the model on text yet\n",
    "        \"\"\"\n",
    "        self.raw_text = text\n",
    "        self.tokens_encoded = self.tokenizer.encode(text, return_tensors=\"pt\")#.to(\"mps\")\n",
    "        self.tokens_decoded = [self.tokenizer.decode(token) for token in self.tokens_encoded[0]]\n",
    "        self._precompute_attns()\n",
    "\n",
    "    def filtered_aggregate_attention(self, inputs, attn, index):\n",
    "        # TODO flag for low info tokens\n",
    "        # print(inputs[0].shape)\n",
    "        #print(inputs)\n",
    "        # n_tokens = len(tokens[1:])\n",
    "        # n_tokens = len(inputs[0][1:])\n",
    "        n_tokens = len(inputs[0][1:])\n",
    "        has_info = np.ones(n_tokens)\n",
    "        if self.drop_low_info_tokens:\n",
    "            # has_info = (inputs[0][1:] != torch.tensor(self.low_info_tokens).reshape(-1, 1)).all(0)\n",
    "            has_info = (inputs[0][1:] != torch.tensor(self.low_info_tokens).reshape(-1, 1)).all(0)\n",
    "        # print(n_tokens, has_info, has_info.shape, inputs.shape)\n",
    "        avged = []\n",
    "        for layer in attn:\n",
    "            # print(\"layer.shape:\", layer.shape)\n",
    "            layer_attns = layer[index-1].squeeze(0)\n",
    "            # print(\"layer_attns.shape:\", layer_attns.shape)\n",
    "            # layer_attns = layer.squeeze(0)\n",
    "            attns_per_head = layer_attns.mean(dim=0)\n",
    "            # print(\"attns_per_head.shape:\", attns_per_head.shape)\n",
    "            # print(\"has_info.shape:\", has_info.shape)\n",
    "            # assert attns_per_head[-1][1:].shape[0] == has_info.shape[0], (attns_per_head[-1][1:].shape, has_info.shape)\n",
    "            # print((attns_per_head[-1][1:].to(\"cpu\") * has_info).shape)\n",
    "            # print(attns_per_head[-1][1:].shape) # 19, 20\n",
    "            # foo = attns_per_head[-1][1:].to(\"cpu\") * has_info\n",
    "            vec = torch.concat((\n",
    "                # We zero the first entry because it's what's called\n",
    "                # null attention (https://aclanthology.org/W19-4808.pdf)\n",
    "                torch.tensor([0.]),\n",
    "                # usually there's only one item in attns_per_head but\n",
    "                # on the first generation, there's a row for each token\n",
    "                # in the prompt as well, so take [-1]\n",
    "                attns_per_head[-1][1:] * has_info,\n",
    "                # generated token gets 0 weight\n",
    "                torch.tensor([0.]),\n",
    "            ))\n",
    "            avged.append(vec / vec.sum())\n",
    "        return torch.stack(avged).mean(dim=0)\n",
    "\n",
    "    def _precompute_attns(self):\n",
    "        n = len(self.tokens_encoded[0])\n",
    "        batched_ids = []\n",
    "        attention_mask = []\n",
    "        batched_ids = [\n",
    "            list(self.tokens_encoded[0,:m]) + [tokenizer.pad_token_id] * (n - m)\n",
    "            for m in range(1, n+1)\n",
    "        ]\n",
    "        attention_mask = [\n",
    "            [1] * m + [0] * (n - m)\n",
    "            for m in range(1, n+1)\n",
    "        ]\n",
    "        outputs = model(\n",
    "            torch.tensor(batched_ids),\n",
    "            attention_mask=torch.tensor(attention_mask),\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        print(\"output_attentions.shape\", len(outputs.attentions), outputs.attentions[0].shape)\n",
    "        # TODO how to use filtered_aggregate_attention???\n",
    "        self.precomputed_attentions = [\n",
    "            self.filtered_aggregate_attention(\n",
    "                self.tokens_encoded,\n",
    "                outputs.attentions,\n",
    "                m,\n",
    "                # outputs.attentions[:,:m,:,:m,:m]\n",
    "            )\n",
    "            for m in range(2, n+1)\n",
    "        ]\n",
    "        return None\n",
    "\n",
    "    def generate_attentions_for_selection(self, selected_start, n_selected=1):\n",
    "        assert n_selected == 1, \"Only one selection supported for now! (TODO)\"\n",
    "        # inputs = self.tokens_encoded[:, 0:(selected_start)]\n",
    "        # print(self.tokens_decoded)\n",
    "        # inputs = self.tokens_decoded[:, 0:selected_start]\n",
    "        inputs = self.tokens_decoded[0:selected_start]\n",
    "        print(\"generate_attns inputs.shape:\", len(inputs))\n",
    "        attn_m = heterogenous_stack(\n",
    "            [\n",
    "                torch.tensor([\n",
    "                    1 if i == j else 0\n",
    "                    for j, token in enumerate(inputs)\n",
    "                ])\n",
    "                for i, token in enumerate(inputs)\n",
    "            ] +\n",
    "            # list(map(aggregate_attention, outputs.attentions))\n",
    "            # [self.filtered_aggregate_attention(inputs, outputs.attentions[0])]\n",
    "            # [self.precomputed_attentions[selected_start+1]]\n",
    "            [self.precomputed_attentions[selected_start-1]]\n",
    "        )\n",
    "        return attn_m\n",
    "\n",
    "    def viz(self, selected_start, n_selected=1):\n",
    "        \"\"\"\n",
    "        Visualize attention for a given selection\n",
    "        \"\"\"\n",
    "        assert n_selected == 1, \"Only one selection supported for now! (TODO)\"\n",
    "        attn_m = self.generate_attentions_for_selection(selected_start, n_selected)\n",
    "\n",
    "        # Create a vector based on selected tokens\n",
    "        # selected_vec = np.zeros(len(self.tokens_encoded[0]))\n",
    "        print(attn_m.shape)\n",
    "        selected_vec = np.zeros(attn_m.shape[0])\n",
    "        selected_vec[selected_start:(selected_start + n_selected)] = 1\n",
    "\n",
    "        # Calculate attention vector\n",
    "        # attn_vec = np.dot(vec, attn_matrix.T)\n",
    "        attn_vec = np.matmul(selected_vec, attn_m)\n",
    "        min_val, max_val = min(attn_vec), max(attn_vec)\n",
    "        attn_vec = (attn_vec - min_val) / (max_val - min_val)\n",
    "        # Generate the HTML code for each token\n",
    "        spans = []\n",
    "\n",
    "        print(selected_vec.shape, attn_m.shape, attn_vec.shape, len(self.tokens_decoded))\n",
    "        for i in range(1, len(self.tokens_decoded)):\n",
    "            token = self.tokens_decoded[i]\n",
    "            # if token == \"<s>\":\n",
    "            #     # think this is a special token. Possibly the start of the prompt?\n",
    "            #     continue\n",
    "            attn = 0\n",
    "            selected = False\n",
    "            if i < attn_vec.shape[0]:\n",
    "                attn = attn_vec[i]\n",
    "                selected = False if i >= len(selected_vec) else selected_vec[i] == 1\n",
    "            underline_style = \"text-decoration: underline;\" if selected else \"\"\n",
    "            spans.append(f'<span style=\"background-color: rgba(255, 0, 0, {attn:.2f}); {underline_style}\">{token}</span>')\n",
    "\n",
    "        # Join the spans and display\n",
    "        display(HTML(' '.join(spans)))\n",
    "\n",
    "        # print(attn_matrix)\n",
    "        print(attn_vec)\n",
    "\n",
    "text_sub = \"DNA computing is an emerging branch of unconventional computing which uses DNA, biochemistry, and molecular\"\n",
    "\n",
    "viz = AttentionVisualizer(model, tokenizer, drop_low_info_tokens=True)\n",
    "viz.fit(text_sub)\n",
    "viz.viz(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09507dc-45ed-4da0-b1e7-446bcb256d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
